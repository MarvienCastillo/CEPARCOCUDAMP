{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675e76ac-3df7-4907-965b-0734ce930776",
   "metadata": {},
   "source": [
    "### Name:\n",
    "> Castillo, Marvien Angel C. <br>\n",
    "> Herrera, Mikaela Gabrielle B. <br>\n",
    "> Regindin, Sean Adrien I. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca5222-52aa-4427-9c3b-af43cc723425",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b62a7b9-a7dd-4fe8-bd35-14496766e694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/bin:/bin:/usr/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory containing the executable to the PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/cuda/bin\"\n",
    "\n",
    "# Check if the directory is added to the PATH\n",
    "print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b00f5-9708-40c2-a38d-0c8e34ab5ce7",
   "metadata": {},
   "source": [
    "# Check if CUDA is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "527bdb74-686d-45e2-af5f-ffa4e5722547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Wed_Apr__9_19:24:57_PDT_2025\n",
      "Cuda compilation tools, release 12.9, V12.9.41\n",
      "Build cuda_12.9.r12.9/compiler.35813241_0\n",
      "nvprof: NVIDIA (R) Cuda command line profiler\n",
      "Copyright (c) 2012 - 2025 NVIDIA Corporation\n",
      "Release version 12.9.19 (21)\n",
      "NVIDIA Nsight Systems version 2025.1.3.140-251335620677v0\n",
      "NVIDIA (R) Nsight Compute Command Line Profiler\n",
      "Copyright (c) 2018-2025 NVIDIA Corporation\n",
      "Version 2025.2.0.0 (build 35613519) (public-release)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvcc --version\n",
    "nvprof --version\n",
    "nsys --version\n",
    "ncu --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d42504cd-6133-426b-8259-bc8b29f494a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  6 12:27:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.51.03              Driver Version: 575.51.03      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           Off |   00000000:00:10.0 Off |                    0 |\n",
      "| N/A   31C    P0             43W /  250W |    1464MiB /  32768MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         1037345      C   ./CUDA_mvp7                             320MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d88540-e0ee-436f-bf98-9ca39515d4e7",
   "metadata": {},
   "source": [
    "# Variant 1 - C Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e971dfd-51ea-442b-9e10-8c56a98da76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing C_var1.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile C_var1.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "// ***C function version\n",
    "void kernel_C(float A[], float B[], float C[], size_t n, int idx[]) {\n",
    "\tfor (int i = 0; i < n; i++) {\n",
    "\t\tif (A[i] >= B[i]) {\n",
    "\t\t\tC[i] = A[i];\n",
    "\t\t\tidx[i] = 0;\n",
    "\t\t}\n",
    "\t\telse {\n",
    "\t\t\tC[i] = B[i];\n",
    "\t\t\tidx[i] = 1;\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "   const size_t ARRAY_SIZE = 1<<28;\n",
    "   const size_t INT_ARRAY_BYTES = ARRAY_SIZE * sizeof(int);\n",
    "   const size_t FLOAT_ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
    "//number of times the program is to be executed\n",
    "   const size_t loope = 30;\n",
    "//declare array\n",
    "   float *C,*A,*B;\n",
    "   int *idx,a;\n",
    "   A = (float*)malloc(FLOAT_ARRAY_BYTES);\n",
    "   B = (float*)malloc(FLOAT_ARRAY_BYTES);\n",
    "   C = (float*)malloc(FLOAT_ARRAY_BYTES);\n",
    "   idx = (int*)malloc(INT_ARRAY_BYTES);\n",
    "   a=2;\n",
    "//timer variables\n",
    "  clock_t start, end;\n",
    "// ***--- initialize your array here ---------\n",
    "   int i;\n",
    "\tfor (i = 0; i < ARRAY_SIZE; i++) {\n",
    "\t\tA[i] = sin(i * 0.0005) * 100.0 + 50.0;\n",
    "\t\tB[i] = cos(i * 0.0003) * 100.0 + 50.0;\n",
    "\t}\n",
    "// fill-in cache\n",
    "    kernel_C(A,B,C,ARRAY_SIZE,idx);\n",
    "//time here\n",
    "  double elapse, time_taken;\n",
    "  elapse = 0.0f;\n",
    "  for (int i=0; i<loope; i++){\n",
    "    start = clock();\n",
    "      kernel_C(A,B,C,ARRAY_SIZE,idx );\n",
    "    end = clock();\n",
    "    time_taken = ((double)(end-start))*1E3/CLOCKS_PER_SEC;\n",
    "    elapse = elapse + time_taken;\n",
    "  }\n",
    "  printf(\"Function (in C) average time for %lu loops is %f milliseconds to execute an array size %lu \\n\", loope, elapse/loope, ARRAY_SIZE);\n",
    "\n",
    "// error checking routine here --\n",
    "   size_t err_count = 0;\n",
    "   int sanity_checker = 0;\n",
    "   for (int i = 0; i < ARRAY_SIZE; i++) { \n",
    "        float expected_C = (A[i] >= B[i]) ? A[i] : B[i];\n",
    "        int expected_idx = (A[i] >= B[i]) ? 0 : 1;\n",
    "    \n",
    "        if (fabs(C[i] - expected_C) > 1e-5 || idx[i] != expected_idx) {\n",
    "            sanity_checker++;\n",
    "        }\n",
    "    }\n",
    "  printf(\"Variant 1 (C) outputs match expected outputs. Errors found = %d.\",sanity_checker);\n",
    "  \n",
    "  // Free memory\n",
    "  free(A);\n",
    "  free(B);\n",
    "  free(C);\n",
    "  free(idx);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2937bac-9c8c-4743-8fdc-a70382f397bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcc C_var1.c -o C_var1 -lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c618468c-6a90-48c3-8f9c-2f718297f72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function (in C) average time for 30 loops is 4648.745000 milliseconds to execute an array size 268435456 \n",
      "Variant 1 (C) outputs match expected outputs. Errors found = 0."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./C_var1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b7521-df56-4266-b663-02eb5447f81f",
   "metadata": {},
   "source": [
    "# Variant 2 - Grid Stride Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffa6ecb3-ed9b-4fdd-a509-392a3dc709e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CUDA_var2.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_var2.cu\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "//Grid stride loop\n",
    "\n",
    "//*** CUDA kernel\n",
    "__global__\n",
    "void kernel(size_t n, float A[],float B[],float C[],int idx[]){\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "    {\n",
    "        if (A[i] >= B[i]) {\n",
    "            C[i] = A[i];\n",
    "            idx[i] = 0;\n",
    "        }\n",
    "        else {\n",
    "            C[i] = B[i];\n",
    "            idx[i] = 1;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(){\n",
    "  const size_t ARRAY_SIZE = 1<<28;\n",
    "  const size_t INT_ARRAY_BYTES = ARRAY_SIZE * sizeof(int);\n",
    "  const size_t FLOAT_ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
    " //number of times the program is to be executed\n",
    "   const size_t loope = 30;\n",
    "//declare array\n",
    "  float *A,*B,*C; \n",
    "  int *idx;\n",
    "  cudaMallocManaged(&A, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&B, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&C, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&idx, INT_ARRAY_BYTES);\n",
    "// *** init array\n",
    "  int i;\n",
    "  for (i = 0; i < ARRAY_SIZE; i++) {\n",
    "    A[i] = sin(i * 0.0005) * 100.0 + 50.0;\n",
    "    B[i] = cos(i * 0.0003) * 100.0 + 50.0;\n",
    "  }\n",
    "\n",
    "// *** setup CUDA kernel\n",
    "  size_t numThreads = 1024;\n",
    "  size_t numBlocks = (ARRAY_SIZE + numThreads-1) / numThreads;\n",
    "\n",
    "  printf(\"*** VARIANT 2 ***\\n\");\n",
    "  printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "  printf(\"numBlocks = %lu, numThreads = %lu \\n\",numBlocks,numThreads);\n",
    "  for (size_t i=0; i<loope;i++)\n",
    "    kernel <<<numBlocks, numThreads>>> (ARRAY_SIZE,A,B,C,idx);\n",
    "//barrier\n",
    "    cudaDeviceSynchronize();\n",
    "  int sanity_checker = 0;\n",
    "  for (int i = 0; i < ARRAY_SIZE; i++) { \n",
    "        float expected_C = (A[i] >= B[i]) ? A[i] : B[i];\n",
    "        int expected_idx = (A[i] >= B[i]) ? 0 : 1;\n",
    "    \n",
    "        if (fabs(C[i] - expected_C) > 1e-5 || idx[i] != expected_idx) {\n",
    "            sanity_checker++;\n",
    "        }\n",
    "    }\n",
    "  printf(\"Variant 2 outputs match expected outputs. Errors found = %d.\",sanity_checker);\n",
    "//free memory\n",
    "  cudaFree(A);\n",
    "  cudaFree(B);\n",
    "  cudaFree(C);\n",
    "  cudaFree(idx);\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c558b99-9904-4214-addc-f54781b834ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_var2.cu -o CUDA_var2 -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc02147f-9af0-4a66-82d7-025fb0c4d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1039328== NVPROF is profiling process 1039328, command: ./CUDA_var2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 2 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 2 outputs match expected outputs. Errors found = 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1039328== Profiling application: ./CUDA_var2\n",
      "==1039328== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  8.35349s        30  278.45ms  11.008ms  7.94593s  kernel(unsigned long, float*, float*, float*, int*)\n",
      "      API calls:   77.15%  8.35394s         1  8.35394s  8.35394s  8.35394s  cudaDeviceSynchronize\n",
      "                   18.67%  2.02125s         4  505.31ms  106.81us  2.01942s  cudaMallocManaged\n",
      "                    4.13%  446.84ms         4  111.71ms  100.13ms  126.71ms  cudaFree\n",
      "                    0.04%  4.7272ms        30  157.57us  7.4860us  3.9101ms  cudaLaunchKernel\n",
      "                    0.01%  565.61us       114  4.9610us     100ns  235.50us  cuDeviceGetAttribute\n",
      "                    0.00%  203.70us         1  203.70us  203.70us  203.70us  cuDeviceGetName\n",
      "                    0.00%  25.400us         1  25.400us  25.400us  25.400us  cuDeviceTotalMem\n",
      "                    0.00%  19.226us         3  6.4080us     248ns  17.669us  cuDeviceGetCount\n",
      "                    0.00%  14.496us         1  14.496us  14.496us  14.496us  cuDeviceGetPCIBusId\n",
      "                    0.00%  6.6200us         2  3.3100us     134ns  6.4860us  cuDeviceGet\n",
      "                    0.00%  1.8190us         1  1.8190us  1.8190us  1.8190us  cuModuleGetLoadingMode\n",
      "                    0.00%  1.5200us         1  1.5200us  1.5200us  1.5200us  cuDeviceGetUuid\n",
      "\n",
      "==1039328== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "   33408  25.920KB  4.0000KB  0.9961MB  845.6445MB  297.6441ms  Host To Device\n",
      "   24572  170.69KB  4.0000KB  0.9961MB  3.999878GB   1.801041s  Device To Host\n",
      "    2744         -         -         -           -   3.325507s  Gpu page fault groups\n",
      "Total CPU Page faults: 18431\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29a543dc-02e3-4149-913f-eab32fab7845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 2 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 2 outputs match expected outputs. Errors found = 0.Collecting data...\n",
      "Generating '/tmp/nsys-report-7a84.qdstrm'\n",
      "[1/1] [========================100%] CUDA_var2.nsys-rep\n",
      "Generated:\n",
      "\t/home/jupyter-mikaela_herrera@dl-90f38/CUDA MP/CUDA_var2.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_var2 ./CUDA_var2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba6431-ecfa-4d1d-8767-9b07a9110d17",
   "metadata": {},
   "source": [
    "# Variant 3.0 - Grid Stride Loop with Prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d448a7f2-2596-42d1-b7aa-4b6618348517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CUDA_var3.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_var3.cu\n",
    "// prefetch\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "//CUDA kernel\n",
    "__global__\n",
    "void kernel(size_t n, float A[],float B[],float C[],int idx[]){\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "    {\n",
    "        if (A[i] >= B[i]) {\n",
    "            C[i] = A[i];\n",
    "            idx[i] = 0;\n",
    "        }\n",
    "        else {\n",
    "            C[i] = B[i];\n",
    "            idx[i] = 1;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(){\n",
    "  const size_t ARRAY_SIZE = 1<<28;\n",
    "  const size_t INT_ARRAY_BYTES = ARRAY_SIZE * sizeof(int);\n",
    "  const size_t FLOAT_ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
    " //number of times the program is to be executed\n",
    "   const size_t loope = 30;\n",
    "//declare array\n",
    "  float *A,*B,*C; \n",
    "  int *idx;\n",
    "  cudaMallocManaged(&A, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&B, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&C, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&idx, INT_ARRAY_BYTES);\n",
    "\n",
    "//get gpu id\n",
    "  int device = -1;\n",
    "  cudaGetDevice(&device);\n",
    "\n",
    "// ****init array\n",
    "  int i;\n",
    "  for (i = 0; i < ARRAY_SIZE; i++) {\n",
    "    A[i] = sin(i * 0.0005) * 100.0 + 50.0;\n",
    "    B[i] = cos(i * 0.0003) * 100.0 + 50.0;\n",
    "  }\n",
    " //\"Prefetch data\" from CPU-GPU\n",
    "  cudaMemPrefetchAsync(A,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(B,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(C,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(idx,INT_ARRAY_BYTES,device,NULL);\n",
    "\n",
    "// setup CUDA kernel\n",
    "    size_t numThreads = 1024;\n",
    "    size_t numBlocks = (ARRAY_SIZE + numThreads-1) / numThreads;\n",
    "\n",
    "  printf(\"*** VARIANT 3 ***\\n\");\n",
    "  printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "  printf(\"numBlocks = %lu, numThreads = %lu \\n\",numBlocks, numThreads);\n",
    "  for (size_t i=0; i<loope;i++)\n",
    "    kernel <<<numBlocks, numThreads>>> (ARRAY_SIZE,A,B,C,idx);\n",
    "//barrier\n",
    "    cudaDeviceSynchronize(); \n",
    "\n",
    "//\"Prefetch data\" from GPU-CPU\n",
    "  cudaMemPrefetchAsync(C,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(A,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(B,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(idx,INT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "\n",
    "  int sanity_checker = 0;\n",
    "  for (int i = 0; i < ARRAY_SIZE; i++) { \n",
    "        float expected_C = (A[i] >= B[i]) ? A[i] : B[i];\n",
    "        int expected_idx = (A[i] >= B[i]) ? 0 : 1;\n",
    "    \n",
    "        if (fabs(C[i] - expected_C) > 1e-5 || idx[i] != expected_idx) {\n",
    "            sanity_checker++;\n",
    "        }\n",
    "    }\n",
    "  printf(\"Variant 3 outputs match expected outputs. Errors found = %d.\",sanity_checker);\n",
    "//free memory\n",
    "  cudaFree(A);\n",
    "  cudaFree(B);\n",
    "  cudaFree(C);\n",
    "  cudaFree(idx);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcc20dc8-3622-46ec-9924-ae5fb7035e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_var3.cu -o CUDA_var3 -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2aafe0fd-a19c-4eb8-865b-284ed0604610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1039804== NVPROF is profiling process 1039804, command: ./CUDA_var3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 3 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 3 outputs match expected outputs. Errors found = 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1039804== Profiling application: ./CUDA_var3\n",
      "==1039804== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  426.11ms        30  14.204ms  11.119ms  17.153ms  kernel(unsigned long, float*, float*, float*, int*)\n",
      "      API calls:   55.88%  3.67313s         8  459.14ms  15.140ms  1.07166s  cudaMemPrefetchAsync\n",
      "                   32.53%  2.13867s         4  534.67ms  88.106us  2.13714s  cudaMallocManaged\n",
      "                    6.50%  427.14ms         1  427.14ms  427.14ms  427.14ms  cudaDeviceSynchronize\n",
      "                    5.02%  330.06ms         4  82.516ms  73.544ms  94.856ms  cudaFree\n",
      "                    0.05%  3.2286ms        30  107.62us  11.453us  2.5755ms  cudaLaunchKernel\n",
      "                    0.01%  658.47us       114  5.7760us     186ns  313.80us  cuDeviceGetAttribute\n",
      "                    0.01%  468.65us         1  468.65us  468.65us  468.65us  cuDeviceGetName\n",
      "                    0.00%  46.639us         1  46.639us  46.639us  46.639us  cuDeviceGetPCIBusId\n",
      "                    0.00%  45.933us         1  45.933us  45.933us  45.933us  cuDeviceTotalMem\n",
      "                    0.00%  30.083us         1  30.083us  30.083us  30.083us  cudaGetDevice\n",
      "                    0.00%  17.044us         2  8.5220us     997ns  16.047us  cuDeviceGet\n",
      "                    0.00%  13.425us         3  4.4750us     214ns  12.877us  cuDeviceGetCount\n",
      "                    0.00%  3.4500us         1  3.4500us  3.4500us  3.4500us  cuModuleGetLoadingMode\n",
      "                    0.00%  1.2430us         1  1.2430us  1.2430us  1.2430us  cuDeviceGetUuid\n",
      "\n",
      "==1039804== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    1024  2.0000MB  2.0000MB  2.0000MB  2.000000GB  323.6496ms  Host To Device\n",
      "    2048  2.0000MB  2.0000MB  2.0000MB  4.000000GB   3.040821s  Device To Host\n",
      "Total CPU Page faults: 6144\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_var3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a72da6f-fbb3-4069-8b06-132db5c165d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 3 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 3 outputs match expected outputs. Errors found = 0.Collecting data...\n",
      "Generating '/tmp/nsys-report-976f.qdstrm'\n",
      "[1/1] [========================100%] CUDA_var3.nsys-rep\n",
      "Generated:\n",
      "\t/home/jupyter-mikaela_herrera@dl-90f38/CUDA MP/CUDA_var3.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_var3 ./CUDA_var3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5373b40-2dbb-40a8-a641-e01f1475922a",
   "metadata": {},
   "source": [
    "# Variant 4.0 - Grid Stride Loop with Prefetch and Page Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "020a7737-1ddb-4031-9e4d-f99222e66b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CUDA_var4.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_var4.cu\n",
    "//prefetch + page creation\n",
    "// page creation responsible gpu fault page\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "//CUDA kernel\n",
    "__global__\n",
    "void kernel(size_t n, float A[],float B[],float C[],int idx[]){\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "    {\n",
    "        if (A[i] >= B[i]) {\n",
    "            C[i] = A[i];\n",
    "            idx[i] = 0;\n",
    "        }\n",
    "        else {\n",
    "            C[i] = B[i];\n",
    "            idx[i] = 1;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "int main(){\n",
    "  const size_t ARRAY_SIZE = 1<<28;\n",
    "  const size_t INT_ARRAY_BYTES = ARRAY_SIZE * sizeof(int);\n",
    "  const size_t FLOAT_ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
    " //number of times the program is to be executed\n",
    "   const size_t loope = 30;\n",
    "//declare array\n",
    "  float *A,*B,*C; \n",
    "  int *idx;\n",
    "  cudaMallocManaged(&A, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&B, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&C, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&idx, INT_ARRAY_BYTES);\n",
    "//get gpu id\n",
    "  int device = -1;\n",
    "  cudaGetDevice(&device);\n",
    "\n",
    "//\"prefetch data\" to create CPU page memory\n",
    "  cudaMemPrefetchAsync(A,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(B,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "//\"prefetch data\" to create GPU page memory\n",
    "  cudaMemPrefetchAsync(C,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(idx,INT_ARRAY_BYTES,device,NULL);\n",
    "\n",
    "// ****init array\n",
    "  int i;\n",
    "  for (i = 0; i < ARRAY_SIZE; i++) {\n",
    "    A[i] = sin(i * 0.0005) * 100.0 + 50.0;\n",
    "    B[i] = cos(i * 0.0003) * 100.0 + 50.0;\n",
    "  }\n",
    "//\"Prefetch data\" from CPU-GPU\n",
    "  cudaMemPrefetchAsync(A,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(B,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(C,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(idx,INT_ARRAY_BYTES,device,NULL);\n",
    "\n",
    "// setup CUDA kernel\n",
    "    size_t numThreads = 1024;   \n",
    "    size_t numBlocks = (ARRAY_SIZE + numThreads-1) / numThreads;\n",
    "\n",
    "  printf(\"*** VARIANT 4 ***\\n\");\n",
    "  printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "  printf(\"numBlocks = %lu, numThreads = %lu \\n\",numBlocks, numThreads);\n",
    "  for (size_t i=0; i<loope;i++)\n",
    "    kernel <<<numBlocks, numThreads>>> (ARRAY_SIZE,A,B,C,idx);\n",
    "//barrier\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "  //\"Prefetch data\" from GPU-CPU\n",
    "  cudaMemPrefetchAsync(C,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(A,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(B,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(idx,INT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "\n",
    "  int sanity_checker = 0;\n",
    "  for (int i = 0; i < ARRAY_SIZE; i++) { \n",
    "        float expected_C = (A[i] >= B[i]) ? A[i] : B[i];\n",
    "        int expected_idx = (A[i] >= B[i]) ? 0 : 1;\n",
    "    \n",
    "        if (fabs(C[i] - expected_C) > 1e-5 || idx[i] != expected_idx) {\n",
    "            sanity_checker++;\n",
    "        }\n",
    "    }\n",
    "  printf(\"Variant 4 outputs match expected outputs. Errors found = %d.\",sanity_checker);\n",
    "\n",
    "//free memory\n",
    "  cudaFree(A);\n",
    "  cudaFree(B);\n",
    "  cudaFree(C);\n",
    "  cudaFree(idx);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "853e10b1-540f-4ed3-aaa1-15c930185f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_var4.cu -o CUDA_var4 -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f28773e4-33f5-4f41-bc23-aa056be9ffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1040071== NVPROF is profiling process 1040071, command: ./CUDA_var4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 4 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 4 outputs match expected outputs. Errors found = 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1040071== Profiling application: ./CUDA_var4\n",
      "==1040071== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  431.24ms        30  14.375ms  11.385ms  18.089ms  kernel(unsigned long, float*, float*, float*, int*)\n",
      "      API calls:   63.52%  5.41810s        12  451.51ms  38.825us  1.17772s  cudaMemPrefetchAsync\n",
      "                   23.29%  1.98677s         4  496.69ms  57.861us  1.98567s  cudaMallocManaged\n",
      "                    5.06%  432.01ms         1  432.01ms  432.01ms  432.01ms  cudaDeviceSynchronize\n",
      "                    4.50%  384.07ms         4  96.017ms  71.549ms  135.56ms  cudaFree\n",
      "                    3.60%  306.87ms        30  10.229ms  11.117us  306.00ms  cudaLaunchKernel\n",
      "                    0.01%  1.2101ms       114  10.614us     169ns  469.79us  cuDeviceGetAttribute\n",
      "                    0.00%  318.67us         1  318.67us  318.67us  318.67us  cuDeviceGetName\n",
      "                    0.00%  132.31us         1  132.31us  132.31us  132.31us  cudaGetDevice\n",
      "                    0.00%  79.372us         1  79.372us  79.372us  79.372us  cuDeviceGetPCIBusId\n",
      "                    0.00%  52.871us         1  52.871us  52.871us  52.871us  cuDeviceTotalMem\n",
      "                    0.00%  25.541us         3  8.5130us     330ns  24.711us  cuDeviceGetCount\n",
      "                    0.00%  9.3540us         2  4.6770us  4.0090us  5.3450us  cuDeviceGet\n",
      "                    0.00%  2.4520us         1  2.4520us  2.4520us  2.4520us  cuModuleGetLoadingMode\n",
      "                    0.00%  1.9820us         1  1.9820us  1.9820us  1.9820us  cuDeviceGetUuid\n",
      "\n",
      "==1040071== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    1024  2.0000MB  2.0000MB  2.0000MB  2.000000GB  359.5312ms  Host To Device\n",
      "    2048  2.0000MB  2.0000MB  2.0000MB  4.000000GB   2.655753s  Device To Host\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_var4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c224b044-ea6d-4a37-a2ac-e8dd9c9794d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 4 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 4 outputs match expected outputs. Errors found = 0.Collecting data...\n",
      "Generating '/tmp/nsys-report-f699.qdstrm'\n",
      "[1/1] [========================100%] CUDA_var4.nsys-rep\n",
      "Generated:\n",
      "\t/home/jupyter-mikaela_herrera@dl-90f38/CUDA MP/CUDA_var4.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_var4 ./CUDA_var4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf27c3d-526d-4752-9109-314ead58f1b0",
   "metadata": {},
   "source": [
    "# Variant 5.0 - Grid Stride Loop with Prefetch and Page Creation + mem advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "164372f7-2a1d-4683-8c52-6607404c7c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CUDA_var5.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_var5.cu\n",
    "//prefetch + page creation + memadvise\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "//CUDA kernel\n",
    "__global__\n",
    "void kernel(size_t n, float A[],float B[],float C[],int idx[]){\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "    {\n",
    "        if (A[i] >= B[i]) {\n",
    "            C[i] = A[i];\n",
    "            idx[i] = 0;\n",
    "        }\n",
    "        else {\n",
    "            C[i] = B[i];\n",
    "            idx[i] = 1;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(){\n",
    "  const size_t ARRAY_SIZE = 1<<28;\n",
    "  const size_t INT_ARRAY_BYTES = ARRAY_SIZE * sizeof(int);\n",
    "  const size_t FLOAT_ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
    " //number of times the program is to be executed\n",
    "   const size_t loope = 30;\n",
    "//declare array\n",
    "  float *A,*B,*C; \n",
    "  int *idx;\n",
    "  cudaMallocManaged(&A, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&B, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&C, FLOAT_ARRAY_BYTES);\n",
    "  cudaMallocManaged(&idx, INT_ARRAY_BYTES);\n",
    "\n",
    "//get gpu id\n",
    "  int device = -1;\n",
    "  cudaGetDevice(&device);\n",
    "\n",
    "// memory advise\n",
    "   cudaMemAdvise(A, FLOAT_ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
    "   cudaMemAdvise(A, FLOAT_ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
    "   cudaMemAdvise(B, FLOAT_ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
    "   cudaMemAdvise(B, FLOAT_ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId);\n",
    "\n",
    "//\"prefetch data\" to create CPU page memory\n",
    "  cudaMemPrefetchAsync(A,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(B,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "//\"prefetch data\" to create GPU page memory\n",
    "  cudaMemPrefetchAsync(C,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(idx,INT_ARRAY_BYTES,device,NULL);\n",
    "\n",
    "// ****init array\n",
    "  int i;\n",
    "  for (i = 0; i < ARRAY_SIZE; i++) {\n",
    "    A[i] = sin(i * 0.0005) * 100.0 + 50.0;\n",
    "    B[i] = cos(i * 0.0003) * 100.0 + 50.0;\n",
    "  }\n",
    "\n",
    "\n",
    " //\"Prefetch data\" from CPU-GPU\n",
    "  cudaMemPrefetchAsync(A,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(B,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(C,FLOAT_ARRAY_BYTES,device,NULL);\n",
    "  cudaMemPrefetchAsync(idx,INT_ARRAY_BYTES,device,NULL);\n",
    "\n",
    "// setup CUDA kernel\n",
    "    size_t numThreads = 1024;\n",
    "    size_t numBlocks = (ARRAY_SIZE + numThreads-1) / numThreads;\n",
    "\n",
    "  printf(\"*** VARIANT 5 ***\\n\");\n",
    "  printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "  printf(\"numBlocks = %lu, numThreads = %lu \\n\",numBlocks, numThreads);\n",
    "  for (size_t i=0; i<loope;i++)\n",
    "    kernel <<<numBlocks, numThreads>>> (ARRAY_SIZE,A,B,C,idx);\n",
    "//barrier\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "  //\"Prefetch data\" from GPU-CPU\n",
    "  cudaMemPrefetchAsync(C,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(A,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(B,FLOAT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "  cudaMemPrefetchAsync(idx,INT_ARRAY_BYTES,cudaCpuDeviceId,NULL);\n",
    "\n",
    "\n",
    "  int sanity_checker = 0;\n",
    "  for (int i = 0; i < ARRAY_SIZE; i++) { \n",
    "        float expected_C = (A[i] >= B[i]) ? A[i] : B[i];\n",
    "        int expected_idx = (A[i] >= B[i]) ? 0 : 1;\n",
    "    \n",
    "        if (fabs(C[i] - expected_C) > 1e-5 || idx[i] != expected_idx) {\n",
    "            sanity_checker++;\n",
    "        }\n",
    "    }\n",
    "  printf(\"Variant 5 outputs match expected outputs. Errors found = %d.\",sanity_checker);\n",
    "\n",
    "//free memory\n",
    "  cudaFree(A);\n",
    "  cudaFree(B);\n",
    "  cudaFree(C);\n",
    "  cudaFree(idx);\n",
    "  return 0;\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6fd4a82f-8d7e-44ff-a297-f144050f2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_var5.cu -o CUDA_var5 -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19dc4c29-d16f-4529-a1c9-36c8ade23ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1040319== NVPROF is profiling process 1040319, command: ./CUDA_var5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 5 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 5 outputs match expected outputs. Errors found = 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1040319== Profiling application: ./CUDA_var5\n",
      "==1040319== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  415.38ms        30  13.846ms  11.007ms  16.976ms  kernel(unsigned long, float*, float*, float*, int*)\n",
      "      API calls:   64.27%  5.03771s        12  419.81ms  5.8213ms  1.36689s  cudaMemPrefetchAsync\n",
      "                   25.90%  2.03017s         4  507.54ms  68.978us  2.02897s  cudaMallocManaged\n",
      "                    5.34%  418.92ms         1  418.92ms  418.92ms  418.92ms  cudaDeviceSynchronize\n",
      "                    4.44%  347.82ms         4  86.956ms  72.836ms  97.685ms  cudaFree\n",
      "                    0.04%  2.9671ms        30  98.902us  7.1010us  2.4522ms  cudaLaunchKernel\n",
      "                    0.01%  516.38us       114  4.5290us     121ns  191.55us  cuDeviceGetAttribute\n",
      "                    0.00%  320.51us         4  80.126us  7.9380us  284.72us  cudaMemAdvise\n",
      "                    0.00%  178.79us         1  178.79us  178.79us  178.79us  cuDeviceGetName\n",
      "                    0.00%  32.491us         1  32.491us  32.491us  32.491us  cudaGetDevice\n",
      "                    0.00%  25.900us         1  25.900us  25.900us  25.900us  cuDeviceTotalMem\n",
      "                    0.00%  19.537us         3  6.5120us     545ns  17.091us  cuDeviceGetCount\n",
      "                    0.00%  14.457us         1  14.457us  14.457us  14.457us  cuDeviceGetPCIBusId\n",
      "                    0.00%  7.0390us         2  3.5190us     152ns  6.8870us  cuDeviceGet\n",
      "                    0.00%     797ns         1     797ns     797ns     797ns  cuModuleGetLoadingMode\n",
      "                    0.00%     273ns         1     273ns     273ns     273ns  cuDeviceGetUuid\n",
      "\n",
      "==1040319== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "    1024  2.0000MB  2.0000MB  2.0000MB  2.000000GB  315.5776ms  Host To Device\n",
      "    1024  2.0000MB  2.0000MB  2.0000MB  2.000000GB   1.621202s  Device To Host\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_var5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88bdaf0f-927f-48bd-8cfc-1211f218943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 5 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 5 outputs match expected outputs. Errors found = 0.Collecting data...\n",
      "Generating '/tmp/nsys-report-54c5.qdstrm'\n",
      "[1/1] [========================100%] CUDA_var5.nsys-rep\n",
      "Generated:\n",
      "\t/home/jupyter-mikaela_herrera@dl-90f38/CUDA MP/CUDA_var5.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_var5 ./CUDA_var5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88115767-5ecc-4555-9db7-1ed69bcf01bc",
   "metadata": {},
   "source": [
    "# Variant 6.0 - CUDA classic MEMCPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a4aabf3-01a1-43d9-9ad2-53b22d5623a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CUDA_var6.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_var6.cu\n",
    "//grid stride loop + memcpy\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "//CUDA kernel\n",
    "__global__\n",
    "void kernel(size_t n, float A[],float B[],float C[],int idx[]){\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride)\n",
    "    {\n",
    "        if (A[i] >= B[i]) {\n",
    "            C[i] = A[i];\n",
    "            idx[i] = 0;\n",
    "        }\n",
    "        else {\n",
    "            C[i] = B[i];\n",
    "            idx[i] = 1;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(){\n",
    "  const size_t ARRAY_SIZE = 1<<28;\n",
    "  const size_t INT_ARRAY_BYTES = ARRAY_SIZE * sizeof(int);\n",
    "  const size_t FLOAT_ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
    " //number of times the program is to be executed\n",
    "   const size_t loope = 30;\n",
    "\n",
    "float *h_A = (float*)malloc(FLOAT_ARRAY_BYTES);\n",
    "float *h_B = (float*)malloc(FLOAT_ARRAY_BYTES);\n",
    "float *h_C = (float*)malloc(FLOAT_ARRAY_BYTES);\n",
    "int *h_idx = (int*)malloc(INT_ARRAY_BYTES);\n",
    "\n",
    "// ****init host array\n",
    "  int i;\n",
    "  for (i = 0; i < ARRAY_SIZE; i++) {\n",
    "    h_A[i] = sinf(i * 0.0005) * 100.0 + 50.0;\n",
    "    h_B[i] = cosf(i * 0.0003) * 100.0 + 50.0;\n",
    "  }\n",
    "\n",
    "// device allocations\n",
    "  float *d_A,*d_B,*d_C;\n",
    "  int *d_idx;\n",
    "  cudaMalloc(&d_A, FLOAT_ARRAY_BYTES); //we dont use cudaMallocManaged with cudaMemcpy\n",
    "  cudaMalloc(&d_B, FLOAT_ARRAY_BYTES);\n",
    "  cudaMalloc(&d_C, FLOAT_ARRAY_BYTES);\n",
    "  cudaMalloc(&d_idx, INT_ARRAY_BYTES);\n",
    "\n",
    "//get gpu id\n",
    "  int device = -1;\n",
    "  cudaGetDevice(&device);\n",
    "\n",
    "// Copy data to device\n",
    "    cudaMemcpy(d_A, h_A, FLOAT_ARRAY_BYTES, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, FLOAT_ARRAY_BYTES, cudaMemcpyHostToDevice);\n",
    "\n",
    "// Kernel launch configuration\n",
    "    size_t threadsPerBlock = 1024;\n",
    "    size_t blocksPerGrid = (ARRAY_SIZE + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    printf(\"*** VARIANT 6 ***\\n\");\n",
    "    printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\",blocksPerGrid, threadsPerBlock);\n",
    "    \n",
    "  for (size_t i=0; i<loope;i++){\n",
    "    kernel <<<blocksPerGrid, threadsPerBlock>>> (ARRAY_SIZE, d_A, d_B, d_C, d_idx);\n",
    "    cudaDeviceSynchronize();\n",
    "  }\n",
    "\n",
    "    \n",
    "// Copy results back to host\n",
    "    cudaMemcpy(h_C, d_C, FLOAT_ARRAY_BYTES, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_idx, d_idx, INT_ARRAY_BYTES, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Verification loop\n",
    "    int errors = 0;\n",
    "    for (int i = 0; i < ARRAY_SIZE; i++) { \n",
    "        float expected_C = (h_A[i] >= h_B[i]) ? h_A[i] : h_B[i];\n",
    "        int expected_idx = (h_A[i] >= h_B[i]) ? 0 : 1;\n",
    "    \n",
    "        if (fabs(h_C[i] - expected_C) > 1e-5 || h_idx[i] != expected_idx) {\n",
    "            errors++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Variant 6 outputs match expected outputs. Errors found = %d.\", errors);\n",
    "\n",
    "// Free device memory\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "    cudaFree(d_idx);\n",
    "\n",
    "// Free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "    free(h_idx);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5719731-db6c-40a0-aabf-ebded2ba59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_var6.cu -o CUDA_var6 -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a975380-63c1-456d-83cd-cc864431ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1040815== NVPROF is profiling process 1040815, command: ./CUDA_var6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 6 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 6 outputs match expected outputs. Errors found = 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1040815== Profiling application: ./CUDA_var6\n",
      "==1040815== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   80.06%  12.8753s         2  6.43766s  6.40197s  6.47335s  [CUDA memcpy DtoH]\n",
      "                   17.84%  2.86827s         2  1.43413s  1.43329s  1.43498s  [CUDA memcpy HtoD]\n",
      "                    2.10%  337.72ms        30  11.257ms  11.112ms  11.381ms  kernel(unsigned long, float*, float*, float*, int*)\n",
      "      API calls:   87.67%  15.7722s         4  3.94306s  1.43505s  6.48485s  cudaMemcpy\n",
      "                   10.02%  1.80179s         4  450.45ms  2.1055ms  1.79446s  cudaMalloc\n",
      "                    2.16%  388.29ms        30  12.943ms  12.608ms  13.281ms  cudaDeviceSynchronize\n",
      "                    0.10%  18.005ms        30  600.16us  244.91us  3.2176ms  cudaLaunchKernel\n",
      "                    0.05%  8.4320ms         4  2.1080ms  1.7182ms  3.0760ms  cudaFree\n",
      "                    0.01%  1.4457ms       114  12.681us     136ns  617.92us  cuDeviceGetAttribute\n",
      "                    0.00%  366.61us         1  366.61us  366.61us  366.61us  cuDeviceGetName\n",
      "                    0.00%  63.186us         1  63.186us  63.186us  63.186us  cuDeviceTotalMem\n",
      "                    0.00%  40.050us         1  40.050us  40.050us  40.050us  cuDeviceGetPCIBusId\n",
      "                    0.00%  22.794us         1  22.794us  22.794us  22.794us  cudaGetDevice\n",
      "                    0.00%  12.681us         2  6.3400us  1.7570us  10.924us  cuDeviceGet\n",
      "                    0.00%  8.9070us         3  2.9690us     210ns  6.7380us  cuDeviceGetCount\n",
      "                    0.00%  2.3740us         1  2.3740us  2.3740us  2.3740us  cuModuleGetLoadingMode\n",
      "                    0.00%  1.8210us         1  1.8210us  1.8210us  1.8210us  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_var6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18e1d470-7aff-492d-bf6b-8616b37080a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 6 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Variant 6 outputs match expected outputs. Errors found = 0.Collecting data...\n",
      "Generating '/tmp/nsys-report-7932.qdstrm'\n",
      "[1/1] [========================100%] CUDA_var6.nsys-rep\n",
      "Generated:\n",
      "\t/home/jupyter-mikaela_herrera@dl-90f38/CUDA MP/CUDA_var6.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_var6 ./CUDA_var6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10850df-63d3-45ae-b1b1-8bd7dcf330f9",
   "metadata": {},
   "source": [
    "# Variant 7.0 - CUDA init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e06b0940-17a1-4bfa-b1ca-361b75af4e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing CUDA_var7.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile CUDA_var7.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "\n",
    "// CUDA kernel to initialize A and B\n",
    "__global__\n",
    "void init_arrays(size_t n, float *A, float *B) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    for (int i = index; i < n; i += stride) {\n",
    "        A[i] = sinf(i * 0.0005f) * 100.0f + 50.0f;\n",
    "        B[i] = cosf(i * 0.0003f) * 100.0f + 50.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const size_t ARRAY_SIZE = 1 << 28;\n",
    "    const size_t FLOAT_BYTES = ARRAY_SIZE * sizeof(float);\n",
    "    //number of times the program is to be executed\n",
    "    const size_t loope = 30;\n",
    "\n",
    "    // Unified memory allocation\n",
    "    float *A, *B;\n",
    "    cudaMallocManaged(&A, FLOAT_BYTES);\n",
    "    cudaMallocManaged(&B, FLOAT_BYTES);\n",
    "\n",
    "\n",
    "    // Launch kernel\n",
    "    size_t threads = 1024;\n",
    "    size_t blocks = (ARRAY_SIZE + threads - 1) / threads;\n",
    "    printf(\"*** VARIANT 7 ***\\n\");\n",
    "    printf(\"numElements = %lu\\n\", ARRAY_SIZE);\n",
    "    printf(\"numBlocks = %lu, numThreads = %lu \\n\",blocks, threads);\n",
    "\n",
    "    for (size_t i=0; i<loope;i++){\n",
    "        init_arrays<<<blocks, threads>>>(ARRAY_SIZE, A, B);\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "    \n",
    "    // Verification loop (no memcpy needed)\n",
    "    int error = 0;\n",
    "    for (int i = 0; i < 100; i++) {\n",
    "        float expected_A = sinf(i * 0.0005f) * 100.0f + 50.0f;\n",
    "        float expected_B = cosf(i * 0.0003f) * 100.0f + 50.0f;\n",
    "        if (fabs(A[i] - expected_A) > 1e-4f || fabs(B[i] - expected_B) > 1e-4f) {\n",
    "            error++;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Errors found = %d\\n\", error);\n",
    "    \n",
    "    // Free unified memory\n",
    "    cudaFree(A);\n",
    "    cudaFree(B);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a1f63e11-88ce-4f2f-8d63-a63773d580ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvcc CUDA_var7.cu -o CUDA_var7 -Wno-deprecated-gpu-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2aadaffd-d7ac-4a43-ad1a-f84444e57565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1041155== NVPROF is profiling process 1041155, command: ./CUDA_var7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 7 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Errors found = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==1041155== Profiling application: ./CUDA_var7\n",
      "==1041155== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  3.62274s        30  120.76ms  9.7476ms  3.33365s  init_arrays(unsigned long, float*, float*)\n",
      "      API calls:   66.01%  3.67450s        30  122.48ms  11.199ms  3.33525s  cudaDeviceSynchronize\n",
      "                   32.83%  1.82750s         2  913.75ms  1.3232ms  1.82618s  cudaMallocManaged\n",
      "                    0.80%  44.468ms         2  22.234ms  20.435ms  24.032ms  cudaFree\n",
      "                    0.34%  18.975ms        30  632.50us  223.77us  5.4305ms  cudaLaunchKernel\n",
      "                    0.02%  885.08us       114  7.7630us     100ns  469.21us  cuDeviceGetAttribute\n",
      "                    0.00%  254.20us         1  254.20us  254.20us  254.20us  cuDeviceGetName\n",
      "                    0.00%  50.951us         1  50.951us  50.951us  50.951us  cuDeviceTotalMem\n",
      "                    0.00%  29.673us         1  29.673us  29.673us  29.673us  cuDeviceGetPCIBusId\n",
      "                    0.00%  6.8430us         2  3.4210us     137ns  6.7060us  cuDeviceGet\n",
      "                    0.00%  5.2980us         3  1.7660us     127ns  5.0380us  cuDeviceGetCount\n",
      "                    0.00%  1.3410us         1  1.3410us  1.3410us  1.3410us  cuModuleGetLoadingMode\n",
      "                    0.00%     526ns         1     526ns     526ns     526ns  cuDeviceGetUuid\n",
      "\n",
      "==1041155== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-32GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "       4  32.000KB  4.0000KB  60.000KB  128.0000KB  36.35200us  Device To Host\n",
      "    2624         -         -         -           -   1.565675s  Gpu page fault groups\n",
      "Total CPU Page faults: 2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvprof ./CUDA_var7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d1622c8-d5b5-4d70-aeb2-39c9435ea521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** VARIANT 7 ***\n",
      "numElements = 268435456\n",
      "numBlocks = 262144, numThreads = 1024 \n",
      "Errors found = 0\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-d023.qdstrm'\n",
      "[1/1] [========================100%] CUDA_var7.nsys-rep\n",
      "Generated:\n",
      "\t/home/jupyter-mikaela_herrera@dl-90f38/CUDA MP/CUDA_var7.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nsys profile  -o CUDA_var7 ./CUDA_var7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da45047-b9c1-4b7b-ba8f-540fef6f1082",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
